{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Merge 4 processed datasets into a single panel ----\n",
    "# Merges on (Country, Year) using OUTER join — keeps missing values as NaN.\n",
    "# Result columns:\n",
    "#   Country | Year | FertilityRate | NetMigration | UrbanPopulationPercentage | FemaleLFP\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths ----\n",
    "cwd = Path.cwd()                 # e.g., .../notebooks\n",
    "BASE_DIR = cwd.parent            # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# ---- Input files (edit names if needed) ----\n",
    "files = {\n",
    "    \"fertility\": PROC_DIR / \"fertility_rate_1960_2018_long_common.csv\",\n",
    "    \"migration\": PROC_DIR / \"net_migration_1960_2018_long_common.csv\",\n",
    "    \"urban_pop\": PROC_DIR / \"urban_population_percentage_1960_2018_long_common.csv\",\n",
    "    \"flfp\":      PROC_DIR / \"female_lfp_rate_1960_2018_long_common.csv\",  \n",
    "}\n",
    "\n",
    "# ---- Helper: detect value column ----\n",
    "def detect_value_column(df):\n",
    "    cols = [c for c in df.columns if c not in (\"Country\", \"Year\")]\n",
    "    if len(cols) != 1:\n",
    "        raise ValueError(f\"Expected exactly 1 value column, found: {cols}\")\n",
    "    return cols[0]\n",
    "\n",
    "# ---- Load all ----\n",
    "dfs = {}\n",
    "for key, path in files.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    val_col = detect_value_column(df)\n",
    "    dfs[key] = df.rename(columns={val_col: key})  # rename value column to key\n",
    "\n",
    "# ---- Merge step by step (outer join on Country & Year) ----\n",
    "merged = dfs[\"fertility\"]\n",
    "for key in [\"migration\", \"urban_pop\", \"flfp\"]:\n",
    "    merged = pd.merge(merged, dfs[key], on=[\"Country\", \"Year\"], how=\"outer\")\n",
    "\n",
    "# ---- Sort and reset index ----\n",
    "merged = merged.sort_values([\"Country\", \"Year\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- Optional: filter to global year range (e.g., 1960–2018) ----\n",
    "# (still keeps NaN values; does not drop missing years)\n",
    "merged = merged[(merged[\"Year\"] >= 1960) & (merged[\"Year\"] <= 2018)]\n",
    "\n",
    "# ---- Save ----\n",
    "out_path = PROC_DIR / \"merged_panel_1960_2018.csv\"\n",
    "merged.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(merged), \"| Columns:\", list(merged.columns))\n",
    "print(\"\\nSample:\")\n",
    "print(merged.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build \"common countries\" across 4 processed datasets ----\n",
    "# After standardization, keep only the countries present in ALL datasets,\n",
    "# then save filtered copies with a *_common.csv suffix + a master countries list.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "import utils_country\n",
    "importlib.reload(utils_country)  # ensure the latest version is used\n",
    "from utils_country import standardize_country_column\n",
    "\n",
    "# ---- Paths ----\n",
    "cwd = Path.cwd()                 # e.g. .../notebooks\n",
    "BASE_DIR = cwd.parent            # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# ---- Helper: drop obvious aggregates if any slipped in ----\n",
    "import re\n",
    "_AGG_REGEX = re.compile(\n",
    "    r\"world|income|area|region|europe|asia|africa|america|caribbean|euro area|\"\n",
    "    r\"sub-?saharan|middle east|north africa|arab world|east asia|south asia|pacific|\"\n",
    "    r\"latin america|and the caribbean|heavily indebted|least developed|small states|\"\n",
    "    r\"fragile and conflict|upper middle|lower middle|high income|low income\",\n",
    "    flags=re.I\n",
    ")\n",
    "def drop_aggregates(df: pd.DataFrame, col=\"Country\") -> pd.DataFrame:\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    mask = df[col].fillna(\"\").astype(str).str.contains(_AGG_REGEX)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "# ---- Configure your processed files here ----\n",
    "# Adjust names if yours differ.\n",
    "files = {\n",
    "    \"fertility\": PROC_DIR / \"fertility_rate_1960_2018_long.csv\",                # Country, Year, FertilityRate\n",
    "    \"migration\": PROC_DIR / \"net_migration_1960_2018_long.csv\",                 # Country, Year, NetMigration\n",
    "    \"urban_pop\": PROC_DIR / \"urban_population_percentage_1960_2018_long.csv\",   # Country, Year, UrbanPopulationPercentage\n",
    "    \"hdi\":       PROC_DIR / \"female_lfp_rate_1960_2018_long.csv\",               # Country, Year, FemaleLFPRate\n",
    "}\n",
    "\n",
    "# Optional per-dataset year windows (HDI commonly starts 1990)\n",
    "year_filters = {\n",
    "    \"fertility\": (1960, 2018),\n",
    "    \"migration\": (1960, 2018),\n",
    "    \"urban_pop\": (1960, 2018),\n",
    "    \"hdi\":       (1990, 2018),\n",
    "}\n",
    "\n",
    "# ---- Load, standardize, and collect sets ----\n",
    "loaded = {}\n",
    "country_sets = {}\n",
    "for key, path in files.items():\n",
    "    if not path.exists():\n",
    "        print(f\"⚠️ Missing file: {path.name} (skipping this dataset)\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if \"Country\" not in df.columns or \"Year\" not in df.columns:\n",
    "        raise ValueError(f\"{path.name} must have columns: Country, Year\")\n",
    "\n",
    "    # Standardize countries and drop aggregates\n",
    "    df = standardize_country_column(df, \"Country\")\n",
    "    df = drop_aggregates(df, \"Country\")\n",
    "\n",
    "    # Optional year filter\n",
    "    if key in year_filters:\n",
    "        y0, y1 = year_filters[key]\n",
    "        df = df[(df[\"Year\"] >= y0) & (df[\"Year\"] <= y1)].copy()\n",
    "\n",
    "    # Deduplicate (Country, Year) just in case\n",
    "    df = df.drop_duplicates(subset=[\"Country\", \"Year\"])\n",
    "\n",
    "    loaded[key] = df\n",
    "    country_sets[key] = set(df[\"Country\"].unique())\n",
    "\n",
    "# ---- Compute intersection across ALL available datasets ----\n",
    "if len(country_sets) < 2:\n",
    "    raise RuntimeError(\"Not enough datasets loaded to compute intersection.\")\n",
    "\n",
    "common_countries = set.intersection(*country_sets.values())\n",
    "print(f\"✅ Common countries across {len(country_sets)} datasets: {len(common_countries)}\")\n",
    "\n",
    "# ---- Save a master list of common countries ----\n",
    "countries_path = PROC_DIR / \"countries_common_across_all.csv\"\n",
    "pd.DataFrame(sorted(common_countries), columns=[\"Country\"]).to_csv(countries_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", countries_path.as_posix())\n",
    "\n",
    "# ---- Filter each dataset to common countries and save *_common.csv ----\n",
    "for key, df in loaded.items():\n",
    "    df_common = df[df[\"Country\"].isin(common_countries)].copy()\n",
    "    out_path = files[key].with_name(files[key].stem + \"_common.csv\")\n",
    "    df_common.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[{key}] kept {len(df_common):,} rows → {out_path.name}\")\n",
    "\n",
    "# ---- Quick peek\n",
    "for key, df in loaded.items():\n",
    "    print(f\"\\n--- {key} sample (common) ---\")\n",
    "    sample = df[df[\"Country\"].isin(common_countries)].head(5)\n",
    "    print(sample.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Migration (SM.POP.NETM.csv) Cleaning\n",
    "# Output schema: Country | Year | NetMigration\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- Paths --------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Country standardization --------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# -------- Helpers --------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Accept year headers like:\n",
    "      - 1960 (int), 1960.0 (float)\n",
    "      - \"1960\" (str), \"1960.0\" (str)\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Normalize year-like header to '####' (e.g., 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Remove aggregate/region rows (e.g., World, income groups).\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust reader for World Bank-style CSVs.\n",
    "    Tries multiple encodings and fallbacks:\n",
    "      utf-8 → latin1 → iso-8859-1 → cp1252\n",
    "    Then various header/structure adjustments.\n",
    "    \"\"\"\n",
    "    # --- Primary encodings to try ---\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Try standard read with several encodings\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Try python engine for messy files\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try skipping World Bank metadata (first 4 lines)\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try semicolon separator\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # --- Last resort: auto-detect header line ---\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Locate country column\n",
    "    country_col = next((c for c in country_col_candidates if c in df.columns), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Prefer not to stringify columns before year detection\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Normalize year headers to '####'\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Year/NA filters\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Project window\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Remove aggregates & canonicalize countries\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# -------- Build --------\n",
    "csv_file = RAW_DIR / \"SM.POP.NETM.csv\"\n",
    "if not csv_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {csv_file}\\nPlace 'SM.POP.NETM.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(csv_file)\n",
    "\n",
    "# In WB format, often there are helper columns we don't need; keep as-is (function handles them).\n",
    "df_netmig = to_three_columns_wide_years(df_raw, value_col_name=\"NetMigration\")\n",
    "\n",
    "# --- NEW: normalize typographic apostrophes before save (Excel-friendly) ---\n",
    "df_netmig[\"Country\"] = (\n",
    "    df_netmig[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# -------- Save --------\n",
    "out_path = PROC_DIR / \"net_migration_1960_2018_long.csv\"\n",
    "# --- NEW: write with UTF-8 BOM so Excel displays Unicode correctly ---\n",
    "df_netmig.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------- Notify --------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_netmig), \" | Columns:\", list(df_netmig.columns))\n",
    "\n",
    "# Optional quick check for unmapped names\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_netmig, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_netmig.head(10))\n",
    "except Exception:\n",
    "    print(df_netmig.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban Population (% of total) Cleaning (UN WUP 2018 - File 21)\n",
    "# Output schema: Country | Year | UrbanPopulationPercentage\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths (project layout) ----------\n",
    "cwd = Path.cwd()                 # e.g., .../notebooks\n",
    "BASE_DIR = cwd.parent            # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Country standardization ----------\n",
    "import importlib, utils_country\n",
    "importlib.reload(utils_country)  # break notebook cache for latest utils\n",
    "from utils_country import standardize_country_column, report_unmapped, canonical_country\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"Accept '1960'(str), '1960.0'(str), 1960(int), or 1960.0(float) as a year column header.\"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Return canonical 4-digit year string for a year-like header (e.g., 1950.0 -> '1950').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Remove aggregate/region rows (e.g., World, income groups).\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\",\n",
    "        \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"Less developed regions\", \"Less developed regions, excluding China\", \"More developed regions\",\n",
    "        \"OCEANIA\", \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\", \"caribbean\", \"euro area\",\n",
    "        \"sub-saharan\", \"middle east\", \"north africa\", \"arab world\", \"east asia\", \"south asia\", \"pacific\",\n",
    "        \"latin america\", \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "# ---------- Excel reader (handles header offset & numeric year headers) ----------\n",
    "def read_population_xlsx(path: Path):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet in xls.sheet_names:\n",
    "        for skip in range(0, 30):  # header row can be offset by intro text\n",
    "            try:\n",
    "                df = pd.read_excel(path, sheet_name=sheet, header=skip)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "            country_col = None\n",
    "            for cand in (\"Region, subregion, country or area\", \"Country Name\", \"Country\", \"Territory\", \"Location\"):\n",
    "                if cand in df.columns:\n",
    "                    country_col = cand\n",
    "                    break\n",
    "            if country_col is None:\n",
    "                continue\n",
    "\n",
    "            year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "            if len(year_cols) >= 3:\n",
    "                keep = [country_col] + year_cols\n",
    "                return df[keep].copy()\n",
    "\n",
    "    raise ValueError(\"No valid sheet found with clear year columns and a country column.\")\n",
    "\n",
    "# ---------- Wide → Long ----------\n",
    "def to_three_columns_population(df: pd.DataFrame, value_col_name=\"UrbanPopulationPercentage\") -> pd.DataFrame:\n",
    "    # find country column\n",
    "    country_col = None\n",
    "    for cand in (\"Region, subregion, country or area\", \"Country Name\", \"Country\", \"Territory\", \"Location\"):\n",
    "        if cand in df.columns:\n",
    "            country_col = cand\n",
    "            break\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found.\")\n",
    "\n",
    "    # normalize year headers to '####' strings\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "\n",
    "    # drop NA, filter to 1960–2018, remove aggregates\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "    # --- canonicalize country names (with demo print) ---\n",
    "    before = long_df[\"Country\"].astype(str).copy()\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "    changed = before != long_df[\"Country\"]\n",
    "    print(f\"[standardize] renamed rows: {int(changed.sum())}\")\n",
    "    if changed.any():\n",
    "        demo = (\n",
    "            pd.DataFrame({\"before\": before[changed], \"after\": long_df.loc[changed, \"Country\"]})\n",
    "            .drop_duplicates()\n",
    "            .head(10)\n",
    "        )\n",
    "        print(demo.to_string(index=False))\n",
    "\n",
    "    # quick sanity for UN short form:\n",
    "    print(\"[sanity] UN short →\", canonical_country(\"Dem. People's Republic of Korea\"))\n",
    "    # expected: Korea, Democratic People’s Republic of\n",
    "\n",
    "    # finalize\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\", \"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\", \"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# ---------- Build ----------\n",
    "xlsx_file = RAW_DIR / \"POPDBWUPRev.20181F21.xlsx\"\n",
    "if not xlsx_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {xlsx_file}\\nPlace 'POPDBWUPRev.20181F21.xlsx' under data/raw/\")\n",
    "\n",
    "df_raw = read_population_xlsx(xlsx_file)\n",
    "df_up = to_three_columns_population(df_raw, value_col_name=\"UrbanPopulationPercentage\")\n",
    "\n",
    "# --- Apostrophe normalization for Excel friendliness (cosmetic) ---\n",
    "df_up[\"Country\"] = (\n",
    "    df_up[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# ---------- Save ----------\n",
    "out_path = PROC_DIR / \"urban_population_percentage_1960_2018_long.csv\"\n",
    "df_up.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_up), \" | Columns:\", list(df_up.columns))\n",
    "\n",
    "# Optional: see unmapped examples to extend the mapping if needed\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_up, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_up.head(10))\n",
    "except Exception:\n",
    "    print(df_up.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female Labor Force Participation (SL.TLF.CACT.FE.ZS.csv) Cleaning\n",
    "# Output schema: Country | Year | FemaleLFPRate\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Country standardization ----------------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Yıl başlıklarını esnek algılar:\n",
    "      - 1960 (int), 1960.0 (float)\n",
    "      - \"1960\" (str), \"1960.0\" (str)\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Yıl başlıklarını 4 haneli stringe çevir (örn. 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Bölgesel/aggregate girdileri ele.\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    WB tarzı CSV'ler için dayanıklı okuyucu.\n",
    "    Sıra: (utf-8 / latin1 / iso-8859-1 / cp1252) → python engine → skiprows=4 → sep=';' → header tarama.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Düz okuma\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Python engine ile\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # WB metaveri (ilk 4 satır) atla\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Noktalı virgül\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Header satırını otomatik bul\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Ülke sütununu bul\n",
    "    country_col = next((c for c in country_col_candidates if c in df.columns), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Yıl sütunlarını tespit et (stringe çevirmeden önce dene)\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Yıl başlıklarını normalize et (1960.0 -> '1960')\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "\n",
    "    # Normalize sonrası yıl listesi (hepsi '####' string)\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Long'a çevir\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Şema standardizasyonu\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Yıl ve boşluk temizliği\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Analiz aralığı\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Bölgeselleri ele\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "    # Ülke adlarını standardize et\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    # Final çıktı\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# ---------------- Build ----------------\n",
    "lfp_file = RAW_DIR / \"SL.TLF.CACT.FE.ZS.csv\"\n",
    "if not lfp_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {lfp_file}\\nPlace 'SL.TLF.CACT.FE.ZS.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(lfp_file)\n",
    "df_f_lfp = to_three_columns_wide_years(df_raw, value_col_name=\"FemaleLFPRate\")\n",
    "\n",
    "# --- Excel'e dost apostrof normalize (kozmetik) ---\n",
    "df_f_lfp[\"Country\"] = (\n",
    "    df_f_lfp[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# ---------------- Save ----------------\n",
    "out_path = PROC_DIR / \"female_lfp_rate_1960_2018_long.csv\"\n",
    "df_f_lfp.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------------- Notify ----------------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_f_lfp), \" | Columns:\", list(df_f_lfp.columns))\n",
    "\n",
    "# Opsiyonel: eşleşmeyen örnekleri gör (harita genişletmek için)\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_f_lfp, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_f_lfp.head(10))\n",
    "except Exception:\n",
    "    print(df_f_lfp.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fertility Rate (SP.DYN.TFRT.IN.csv) Cleaning\n",
    "# Output schema: Country | Year | FertilityRate\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- Relative Paths --------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Country standardization --------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# -------- Helpers --------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Accept '1960' (str), '1960.0' (str), 1960 (int), or 1960.0 (float) as a year column header.\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Normalize year-like headers to 4-digit strings (e.g., 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Drop aggregates/regions that aren't individual countries.\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust reader for World Bank-style CSVs.\n",
    "    Tries multiple encodings and fallbacks:\n",
    "      utf-8 → latin1 → iso-8859-1 → cp1252\n",
    "    Then various header/structure adjustments.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Try standard read\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Try python engine for messy files\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try skipping World Bank metadata (first 4 lines)\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try semicolon separator\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Last resort: auto-detect header line\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Locate country column\n",
    "    country_col = next((c for c in df.columns if c in country_col_candidates), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Detect year columns without forcing all to str first\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Normalize year headers like \"1960.0\" -> \"1960\"\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Keep only numeric years and drop missing\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Filter analysis window\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Remove aggregates\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "    # Canonicalize country names\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    # Final ordering and schema\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# -------- Creation --------\n",
    "fertility_file = RAW_DIR / \"SP.DYN.TFRT.IN.csv\"\n",
    "if not fertility_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {fertility_file}\\nPlace 'SP.DYN.TFRT.IN.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(fertility_file)\n",
    "\n",
    "# -------- Build 3-column dataset --------\n",
    "df_fert = to_three_columns_wide_years(df_raw, value_col_name=\"FertilityRate\")\n",
    "\n",
    "# --- Excel-friendly apostrophe normalize (optional but nice) ---\n",
    "df_fert[\"Country\"] = (\n",
    "    df_fert[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# -------- Save --------\n",
    "out_path = PROC_DIR / \"fertility_rate_1960_2018_long.csv\"\n",
    "df_fert.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------- Notification --------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_fert), \"| Columns:\", list(df_fert.columns))\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_fert, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_fert.head(10))\n",
    "except Exception:\n",
    "    print(df_fert.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
