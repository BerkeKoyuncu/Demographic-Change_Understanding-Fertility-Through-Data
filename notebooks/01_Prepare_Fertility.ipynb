{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Migration (SM.POP.NETM.csv) Cleaning\n",
    "# Output schema: Country | Year | NetMigration\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- Paths --------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Country standardization --------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# -------- Helpers --------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Accept year headers like:\n",
    "      - 1960 (int), 1960.0 (float)\n",
    "      - \"1960\" (str), \"1960.0\" (str)\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Normalize year-like header to '####' (e.g., 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Remove aggregate/region rows (e.g., World, income groups).\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust reader for World Bank-style CSVs.\n",
    "    Tries multiple encodings and fallbacks:\n",
    "      utf-8 → latin1 → iso-8859-1 → cp1252\n",
    "    Then various header/structure adjustments.\n",
    "    \"\"\"\n",
    "    # --- Primary encodings to try ---\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Try standard read with several encodings\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Try python engine for messy files\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try skipping World Bank metadata (first 4 lines)\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try semicolon separator\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # --- Last resort: auto-detect header line ---\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Locate country column\n",
    "    country_col = next((c for c in country_col_candidates if c in df.columns), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Prefer not to stringify columns before year detection\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Normalize year headers to '####'\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Year/NA filters\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Project window\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Remove aggregates & canonicalize countries\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# -------- Build --------\n",
    "csv_file = RAW_DIR / \"SM.POP.NETM.csv\"\n",
    "if not csv_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {csv_file}\\nPlace 'SM.POP.NETM.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(csv_file)\n",
    "\n",
    "# In WB format, often there are helper columns we don't need; keep as-is (function handles them).\n",
    "df_netmig = to_three_columns_wide_years(df_raw, value_col_name=\"NetMigration\")\n",
    "\n",
    "# --- NEW: normalize typographic apostrophes before save (Excel-friendly) ---\n",
    "df_netmig[\"Country\"] = (\n",
    "    df_netmig[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# -------- Save --------\n",
    "out_path = PROC_DIR / \"net_migration_1960_2018_long.csv\"\n",
    "# --- NEW: write with UTF-8 BOM so Excel displays Unicode correctly ---\n",
    "df_netmig.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------- Notify --------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_netmig), \" | Columns:\", list(df_netmig.columns))\n",
    "\n",
    "# Optional quick check for unmapped names\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_netmig, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_netmig.head(10))\n",
    "except Exception:\n",
    "    print(df_netmig.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban Population (% of total) Cleaning (UN WUP 2018 - File 21)\n",
    "# Output schema: Country | Year | UrbanPopulationPercentage\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths (project layout) ----------\n",
    "cwd = Path.cwd()                 # e.g., .../notebooks\n",
    "BASE_DIR = cwd.parent            # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Country standardization ----------\n",
    "import importlib, utils_country\n",
    "importlib.reload(utils_country)  # notebokta eski sürüm cache’ini kır\n",
    "from utils_country import standardize_country_column, report_unmapped, canonical_country\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"Accept '1960'(str), '1960.0'(str), 1960(int), or 1960.0(float) as a year column header.\"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Return canonical 4-digit year string for a year-like header (e.g., 1950.0 -> '1950').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Remove aggregate/region rows (e.g., World, income groups).\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"Less developed regions\", \"Less developed regions, excluding China\", \"More developed regions\", \"OCEANIA\", \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "# ---------- Excel reader (handles header offset & numeric year headers) ----------\n",
    "def read_population_xlsx(path: Path):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet in xls.sheet_names:\n",
    "        for skip in range(0, 30):  # header row can be offset by intro text\n",
    "            try:\n",
    "                df = pd.read_excel(path, sheet_name=sheet, header=skip)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "            country_col = None\n",
    "            for cand in (\"Region, subregion, country or area\", \"Country Name\", \"Country\", \"Territory\", \"Location\"):\n",
    "                if cand in df.columns:\n",
    "                    country_col = cand\n",
    "                    break\n",
    "            if country_col is None:\n",
    "                continue\n",
    "\n",
    "            year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "            if len(year_cols) >= 3:\n",
    "                keep = [country_col] + year_cols\n",
    "                return df[keep].copy()\n",
    "\n",
    "    raise ValueError(\"No valid sheet found with clear year columns and a country column.\")\n",
    "\n",
    "# ---------- Wide → Long ----------\n",
    "def to_three_columns_population(df: pd.DataFrame, value_col_name=\"UrbanPopulationPercentage\") -> pd.DataFrame:\n",
    "    # find country column\n",
    "    country_col = None\n",
    "    for cand in (\"Region, subregion, country or area\", \"Country Name\", \"Country\", \"Territory\", \"Location\"):\n",
    "        if cand in df.columns:\n",
    "            country_col = cand\n",
    "            break\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found.\")\n",
    "\n",
    "    # normalize year headers to '####' strings\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "\n",
    "    # drop NA, filter to 1960–2018, remove aggregates\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "        # --- canonicalize country names (proof print) ---\n",
    "    before = long_df[\"Country\"].astype(str).copy()\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "    changed = before != long_df[\"Country\"]\n",
    "    print(f\"[standardize] renamed rows: {int(changed.sum())}\")\n",
    "    if changed.any():\n",
    "        demo = (\n",
    "            pd.DataFrame({\"before\": before[changed], \"after\": long_df.loc[changed, \"Country\"]})\n",
    "            .drop_duplicates()\n",
    "            .head(10)\n",
    "        )\n",
    "        print(demo.to_string(index=False))\n",
    "\n",
    "    # quick sanity for UN short form:\n",
    "    print(\"[sanity] UN short →\", canonical_country(\"Dem. People's Republic of Korea\"))\n",
    "    # beklenen: Korea, Democratic People’s Republic of\n",
    "\n",
    "\n",
    "    # finalize\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\", \"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\", \"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# ---------- Build ----------\n",
    "xlsx_file = RAW_DIR / \"POPDBWUPRev.20181F21.xlsx\"\n",
    "if not xlsx_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {xlsx_file}\\nPlace 'POPDBWUPRev.20181F21.xlsx' under data/raw/\")\n",
    "\n",
    "df_raw = read_population_xlsx(xlsx_file)\n",
    "df_up = to_three_columns_population(df_raw, value_col_name=\"UrbanPopulationPercentage\")\n",
    "\n",
    "# --- Excel'e dost apostrof normalize (kozmetik) ---\n",
    "df_up[\"Country\"] = (\n",
    "    df_up[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# ---------- Save ----------\n",
    "out_path = PROC_DIR / \"urban_population_percentage_1960_2018_long.csv\"\n",
    "df_up.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_up), \" | Columns:\", list(df_up.columns))\n",
    "\n",
    "# Optional: see unmapped examples to extend the mapping if needed\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_up, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_up.head(10))\n",
    "except Exception:\n",
    "    print(df_up.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female Labor Force Participation (SL.TLF.CACT.FE.ZS.csv) Cleaning\n",
    "# Output schema: Country | Year | FemaleLFPRate\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Country standardization ----------------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Yıl başlıklarını esnek algılar:\n",
    "      - 1960 (int), 1960.0 (float)\n",
    "      - \"1960\" (str), \"1960.0\" (str)\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Yıl başlıklarını 4 haneli stringe çevir (örn. 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Bölgesel/aggregate girdileri ele.\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    WB tarzı CSV'ler için dayanıklı okuyucu.\n",
    "    Sıra: (utf-8 / latin1 / iso-8859-1 / cp1252) → python engine → skiprows=4 → sep=';' → header tarama.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Düz okuma\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Python engine ile\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # WB metaveri (ilk 4 satır) atla\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Noktalı virgül\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Header satırını otomatik bul\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Ülke sütununu bul\n",
    "    country_col = next((c for c in country_col_candidates if c in df.columns), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Yıl sütunlarını tespit et (stringe çevirmeden önce dene)\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Yıl başlıklarını normalize et (1960.0 -> '1960')\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "\n",
    "    # Normalize sonrası yıl listesi (hepsi '####' string)\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Long'a çevir\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Şema standardizasyonu\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Yıl ve boşluk temizliği\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Analiz aralığı\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Bölgeselleri ele\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "    # Ülke adlarını standardize et\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    # Final çıktı\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# ---------------- Build ----------------\n",
    "lfp_file = RAW_DIR / \"SL.TLF.CACT.FE.ZS.csv\"\n",
    "if not lfp_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {lfp_file}\\nPlace 'SL.TLF.CACT.FE.ZS.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(lfp_file)\n",
    "df_f_lfp = to_three_columns_wide_years(df_raw, value_col_name=\"FemaleLFPRate\")\n",
    "\n",
    "# --- Excel'e dost apostrof normalize (kozmetik) ---\n",
    "df_f_lfp[\"Country\"] = (\n",
    "    df_f_lfp[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# ---------------- Save ----------------\n",
    "out_path = PROC_DIR / \"female_lfp_rate_1960_2018_long.csv\"\n",
    "df_f_lfp.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------------- Notify ----------------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_f_lfp), \" | Columns:\", list(df_f_lfp.columns))\n",
    "\n",
    "# Opsiyonel: eşleşmeyen örnekleri gör (harita genişletmek için)\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_f_lfp, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_f_lfp.head(10))\n",
    "except Exception:\n",
    "    print(df_f_lfp.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fertility Rate (SP.DYN.TFRT.IN.csv) Cleaning\n",
    "# Output schema: Country | Year | FertilityRate\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- Relative Paths --------\n",
    "cwd = Path.cwd()            # .../notebooks\n",
    "BASE_DIR = cwd.parent       # project root\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Country standardization --------\n",
    "from utils_country import standardize_country_column, report_unmapped\n",
    "\n",
    "# -------- Helpers --------\n",
    "def is_year_col(col) -> bool:\n",
    "    \"\"\"\n",
    "    Accept '1960' (str), '1960.0' (str), 1960 (int), or 1960.0 (float) as a year column header.\n",
    "    \"\"\"\n",
    "    if isinstance(col, (int, float)):\n",
    "        try:\n",
    "            y = int(col)\n",
    "            return 1900 <= y <= 2100\n",
    "        except Exception:\n",
    "            return False\n",
    "    s = str(col).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        y = int(float(s))\n",
    "        return 1900 <= y <= 2100\n",
    "    return False\n",
    "\n",
    "def normalize_year_header(c):\n",
    "    \"\"\"Normalize year-like headers to 4-digit strings (e.g., 1960.0 -> '1960').\"\"\"\n",
    "    if isinstance(c, (int, float)):\n",
    "        return str(int(c))\n",
    "    s = str(c).strip()\n",
    "    if re.fullmatch(r\"\\d{4}(?:\\.0+)?\", s):\n",
    "        return str(int(float(s)))\n",
    "    return s\n",
    "\n",
    "def clean_country_name(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip()\n",
    "\n",
    "def drop_non_countries(df: pd.DataFrame, country_col: str = \"Country\") -> pd.DataFrame:\n",
    "    \"\"\"Drop aggregates/regions that aren't individual countries.\"\"\"\n",
    "    if country_col not in df.columns:\n",
    "        return df\n",
    "    drop_keywords = [\n",
    "        \"Early-demographic dividend\", \"IBRD only\", \"IDA & IBRD total\", \"IDA blend\", \"IDA only\", \"IDA total\", \"Late-demographic dividend\", \"OECD members\", \"Post-demographic dividend\", \"Pre-demographic dividend\",\n",
    "        \"income\", \"world\", \"europe\", \"asia\", \"africa\", \"america\",\n",
    "        \"caribbean\", \"euro area\", \"sub-saharan\", \"middle east\", \"north africa\",\n",
    "        \"arab world\", \"east asia\", \"south asia\", \"pacific\", \"latin america\",\n",
    "        \"and the caribbean\", \"heavily indebted\", \"least developed\", \"small states\",\n",
    "        \"fragile and conflict\", \"upper middle\", \"lower middle\", \"high income\", \"low income\"\n",
    "    ]\n",
    "    patt = re.compile(\"|\".join([re.escape(k) for k in drop_keywords]), flags=re.I)\n",
    "    mask = df[country_col].fillna(\"\").astype(str).str.contains(patt)\n",
    "    return df.loc[~mask].copy()\n",
    "\n",
    "def read_any_csv_first(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust reader for World Bank-style CSVs.\n",
    "    Tries multiple encodings and fallbacks:\n",
    "      utf-8 → latin1 → iso-8859-1 → cp1252\n",
    "    Then various header/structure adjustments.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"cp1252\"]\n",
    "\n",
    "    # Try standard read\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "\n",
    "    # Try python engine for messy files\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\",\n",
    "                               on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try skipping World Bank metadata (first 4 lines)\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, skiprows=4, low_memory=False)\n",
    "        except (pd.errors.ParserError, UnicodeDecodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try semicolon separator\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, sep=\";\", skiprows=4,\n",
    "                               engine=\"python\", on_bad_lines=\"skip\", low_memory=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Last resort: auto-detect header line\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_idx = next(\n",
    "        (i for i, line in enumerate(lines)\n",
    "         if line.lower().startswith(\"country name\") or line.lower().startswith(\"country\")),\n",
    "        0\n",
    "    )\n",
    "    return pd.read_csv(path, header=header_idx, engine=\"python\",\n",
    "                       on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "def to_three_columns_wide_years(df: pd.DataFrame,\n",
    "                                country_col_candidates=(\"Country Name\",\"Country\",\"Territory\",\"Location\"),\n",
    "                                value_col_name=\"Value\") -> pd.DataFrame:\n",
    "    # Locate country column\n",
    "    country_col = next((c for c in df.columns if c in country_col_candidates), None)\n",
    "    if country_col is None:\n",
    "        raise ValueError(\"Country column not found. Update 'country_col_candidates' if needed.\")\n",
    "\n",
    "    # Detect year columns without forcing all to str first\n",
    "    year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "    if not year_cols:\n",
    "        df = df.rename(columns={c: str(c) for c in df.columns})\n",
    "        year_cols = [c for c in df.columns if is_year_col(c)]\n",
    "        if not year_cols:\n",
    "            raise ValueError(\"No year columns detected (e.g., 1960, 1961, ...). Is this the correct file?\")\n",
    "\n",
    "    # Normalize year headers like \"1960.0\" -> \"1960\"\n",
    "    df = df.rename(columns={c: normalize_year_header(c) for c in df.columns})\n",
    "    year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    # Melt to long\n",
    "    long_df = df.melt(id_vars=[country_col], value_vars=year_cols,\n",
    "                      var_name=\"Year\", value_name=value_col_name)\n",
    "\n",
    "    # Standardize schema\n",
    "    long_df = long_df.rename(columns={country_col: \"Country\"})\n",
    "    long_df[\"Country\"] = long_df[\"Country\"].map(clean_country_name)\n",
    "\n",
    "    # Keep only numeric years and drop missing\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df = long_df.dropna(subset=[value_col_name])\n",
    "\n",
    "    # Filter analysis window\n",
    "    long_df = long_df[(long_df[\"Year\"] >= 1960) & (long_df[\"Year\"] <= 2018)]\n",
    "\n",
    "    # Remove aggregates\n",
    "    long_df = drop_non_countries(long_df, \"Country\")\n",
    "\n",
    "    # Canonicalize country names\n",
    "    long_df = standardize_country_column(long_df, col=\"Country\")\n",
    "\n",
    "    # Final ordering and schema\n",
    "    out = long_df[[\"Country\", \"Year\", value_col_name]].sort_values([\"Country\",\"Year\"]).reset_index(drop=True)\n",
    "    assert set(out.columns) == {\"Country\",\"Year\", value_col_name}\n",
    "    return out\n",
    "\n",
    "# -------- Creation --------\n",
    "fertility_file = RAW_DIR / \"SP.DYN.TFRT.IN.csv\"\n",
    "if not fertility_file.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {fertility_file}\\nPlace 'SP.DYN.TFRT.IN.csv' under data/raw/\")\n",
    "\n",
    "df_raw = read_any_csv_first(fertility_file)\n",
    "\n",
    "# -------- Build 3-column dataset --------\n",
    "df_fert = to_three_columns_wide_years(df_raw, value_col_name=\"FertilityRate\")\n",
    "\n",
    "# --- Excel-friendly apostrophe normalize (optional but nice) ---\n",
    "df_fert[\"Country\"] = (\n",
    "    df_fert[\"Country\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u2019\\u2018\\u2032\\u00B4`]\", \"'\", regex=True)\n",
    ")\n",
    "\n",
    "# -------- Save --------\n",
    "out_path = PROC_DIR / \"fertility_rate_1960_2018_long.csv\"\n",
    "df_fert.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------- Notification --------\n",
    "print(\"✅ Saved:\", out_path.as_posix())\n",
    "print(\"Rows:\", len(df_fert), \"| Columns:\", list(df_fert.columns))\n",
    "try:\n",
    "    print(\"Unmapped (sample):\")\n",
    "    print(report_unmapped(df_fert, \"Country\", sample=20))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_fert.head(10))\n",
    "except Exception:\n",
    "    print(df_fert.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
